<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation.">
    <meta name="keywords" content="CUDA Agent, CUDA, RL, kernel generation, KernelBench, GPU optimization">
    <meta name="author" content="CUDA Agent Team - ByteDance Seed, AIR Tsinghua, SIA-Lab">
    <title>CUDA Agent | Large-Scale Agentic RL for CUDA Kernel Generation</title>

    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/hero.css">
    <link rel="stylesheet" href="css/animations.css">
    <link rel="stylesheet" href="css/cuda-page.css">
</head>
<body>
    <a href="#main-content" class="skip-to-content">Skip to main content</a>

    <div class="logo-bar">
        <div class="logo-container">
            <img src="asset/seed_logo.png" alt="ByteDance Seed">
            <img src="asset/AIRlogo.png" alt="Institute for AI Industry Research, Tsinghua University">
        </div>
    </div>

    <section class="hero" id="hero">
        <div class="hero-particles" id="particles"></div>
        <div class="hero-content">
            <div class="hero-brand">
                <h1 class="hero-title">CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation</h1>
                <div class="flex-logo">
                    <span>CUDA Agent</span>
                </div>
                <div class="hero-tagline">
                    <span class="tagline-text">High-Quality Training Tasks via a Scalable Data Pipeline</span>
                </div>
            </div>

            <p class="hero-description">
                CUDA Agent is a large-scale agentic reinforcement learning system that develops robust CUDA kernel optimization ability
                through scalable data synthesis, a skill-augmented execution environment, and stable long-horizon RL training.
            </p>

            <div class="authors">
                <span class="author-block">Weinan Dai<sup>1,2,3</sup><sup>*</sup>,</span>
                <span class="author-block">Hanlin Wu<sup>1,2,3</sup><sup>*</sup>,</span>
                <span class="author-block">Qiying Yu<sup>1,2,3</sup>,</span>
                <span class="author-block">Huan-ang Gao<sup>1,2,3</sup>,</span>
                <span class="author-block">Jiahao Li<sup>1</sup>,</span>
                <span class="author-block">Chengquan Jiang<sup>1</sup>,</span>
                <span class="author-block">Weiqiang Lou<sup>1</sup>,</span>
                <span class="author-block">Yufan Song<sup>1</sup>,</span>
                <span class="author-block">Hongli Yu<sup>1,2,3</sup>,</span>
                <span class="author-block">Jiaze Chen<sup>1,3</sup>,</span>
                <span class="author-block">Wei-Ying Ma<sup>2,3</sup>,</span>
                <span class="author-block">Ya-Qin Zhang<sup>2,3</sup>,</span>
                <span class="author-block">Jingjing Liu<sup>2,3</sup>,</span>
                <span class="author-block">Mingxuan Wang<sup>1,3</sup>,</span>
                <span class="author-block">Xin Liu<sup>1</sup>,</span>
                <span class="author-block">Hao Zhou<sup>2,3</sup><sup>&dagger;</sup></span>
            </div>
            <div class="affiliations">
                <div><sup>1</sup>ByteDance Seed</div>
                <div><sup>2</sup>Institute for AI Industry Research (AIR), Tsinghua University</div>
                <div><sup>3</sup>SIA-Lab of Tsinghua AIR and ByteDance Seed</div>
                <div class="hero-micro">* Equal contributions, ‚Ä† Corresponding Authors</div>
            </div>

            <div class="hero-cta">
                <a href="./static/pdf/CUDA_Agent_Arxiv_Version.pdf" class="btn btn-primary" target="_blank" rel="noopener">
                    <span>Paper (PDF)</span>
                    <span class="btn-icon">üìÑ</span>
                </a>
                <a href="https://github.com/BytedTsinghua-SIA/CUDA-Agent" class="btn btn-secondary" target="_blank" rel="noopener">
                    <span>GitHub</span>
                    <span class="btn-icon">‚≠ê</span>
                </a>
                <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/CUDA-Agent-Ops-6K" class="btn btn-secondary" target="_blank" rel="noopener">
                    <span>Dataset</span>
                    <span class="btn-icon">ü§ó</span>
                </a>
            </div>

            <div class="hero-stats">
                <div class="stat-card">
                    <span class="stat-number">98.8%</span>
                    <span class="stat-label">Overall Pass Rate</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">96.8%</span>
                    <span class="stat-label">Overall Faster than torch.compile</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">2.11x</span>
                    <span class="stat-label">Overall Speedup vs torch.compile</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">6K</span>
                    <span class="stat-label">Synthesized Training Ops</span>
                </div>
            </div>

            <a class="scroll-indicator" href="#news" aria-label="Scroll to content">
                <span>Scroll to details</span>
                <span class="scroll-indicator-icon">‚Üì</span>
            </a>
        </div>
    </section>

    <nav role="navigation" aria-label="Main navigation">
        <a href="#news">News</a>
        <a href="#abstract">Abstract</a>
        <a href="#contributions">Contributions</a>
        <a href="#pipeline">Pipeline</a>
        <a href="#main-results">Results</a>
        <a href="#citation">Citation</a>
    </nav>

    <main id="main-content">
        <div class="container">
            <section id="news" class="news-section">
                <h2>Latest News</h2>
                <div class="news-item">
                    <span class="news-date">2026.02.27</span>
                    The <a href="https://github.com/BytedTsinghua-SIA/CUDA-Agent" target="_blank" rel="noopener">GitHub repository</a>
                    now includes the agent workdir for the CUDA Agent workflow.
                </div>
                <div class="news-item">
                    <span class="news-date">2026.02.27</span>
                    The training dataset has been released on
                    <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/CUDA-Agent-Ops-6K" target="_blank" rel="noopener">Hugging Face</a>
                    as CUDA-Agent-Ops-6K.
                </div>
            </section>

            <section id="abstract">
                <h2>Abstract</h2>
                <p>
                    GPU kernel optimization is fundamental to modern deep learning but remains a specialized task requiring
                    deep hardware expertise. Existing CUDA code generation approaches either rely on training-free refinement
                    or fixed execution-feedback loops, which limits intrinsic optimization ability.
                </p>
                <p>
                    We present CUDA Agent, a large-scale agentic reinforcement learning system with three core components:
                    scalable data synthesis, a skill-augmented CUDA development environment with reliable verification and profiling,
                    and RL algorithmic techniques for stable long-context training.
                </p>
                <p>
                    CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100%, 100%, and 92% faster rate
                    over torch.compile on Level-1, Level-2, and Level-3 splits.
                </p>
                <div class="figure-wrap">
                    <img src="static/images/benchmark_chart.png" alt="KernelBench benchmark chart for CUDA Agent">
                    <p class="figure-caption">KernelBench comparison against torch.compile and strong proprietary models.</p>
                </div>
            </section>

            <section id="contributions">
                <h2>Key Contributions</h2>
                <div class="grid-3">
                    <div class="card">
                        <h3>Large-Scale Agentic RL System for CUDA Optimization</h3>
                        <p>
                            We introduce CUDA Agent, a large-scale agentic reinforcement learning system that improves
                            intrinsic CUDA kernel generation and optimization ability through scalable synthesis,
                            a skill-augmented environment, and stable long-horizon training.
                        </p>
                    </div>
                    <div class="card">
                        <h3>State-of-the-Art KernelBench Performance</h3>
                        <p>
                            CUDA Agent achieves state-of-the-art results on KernelBench, delivering strong faster-than-compile
                            rates across all levels and outperforming strong proprietary models on the hardest Level-3 setting.
                        </p>
                    </div>
                    <div class="card">
                        <h3>Data Release: CUDA-Agent-Ops-6K</h3>
                        <p>
                            We release CUDA-Agent-Ops-6K, a high-quality synthesized training dataset with filtering and
                            contamination control, supporting reproducible research on RL-based CUDA kernel optimization.
                        </p>
                    </div>
                </div>
            </section>

            <section id="pipeline">
                <h2>System Pipeline</h2>
                <div class="pipeline-stack">
                    <div class="card">
                        <h3>Data Synthesis</h3>
                        <p>
                            We build training tasks with a three-stage pipeline: seed problem crawling, LLM-based combinatorial
                            synthesis, and execution-driven filtering. Seed operators are mined from <code>torch</code> and
                            <code>transformers</code>, each represented as a Python class with initialization and forward methods.
                        </p>
                        <ul class="pipeline-list">
                            <li>Combinatorial synthesis samples up to 5 torch operators and composes them sequentially into fused tasks.</li>
                            <li>Filtering keeps only tasks that run in both eager and compile modes and removes stochastic operators.</li>
                            <li>Anti-hacking checks remove constant or indistinguishable outputs across different inputs.</li>
                            <li>Workload control keeps eager runtime in the 1ms-100ms range and removes high-similarity KernelBench cases.</li>
                        </ul>
                        <p>
                            The final curated dataset contains 6,000 training samples (CUDA-Agent-Ops-6K), designed for scalable
                            RL training with broad task diversity and reduced contamination risk.
                        </p>
                        <div class="figure-wrap">
                            <img src="static/images/data_pipeline.png" alt="CUDA Agent data synthesis pipeline">
                        </div>
                    </div>
                    <div class="card">
                        <h3>Agent Environment</h3>
                        <p>
                            The agent loop follows a ReAct-style workflow with coding tools and a CUDA skill specification
                            (SKILL.md), enabling iterative coding, compile-debug cycles, and profiler-guided optimization.
                        </p>
                        <ul class="pipeline-list">
                            <li>Standard workflow: profile native PyTorch, implement CUDA kernels/bindings, compile in GPU sandbox, iterate.</li>
                            <li>Target requirement: pass correctness checks and exceed a 5% speedup over <code>torch.compile</code>.</li>
                            <li>Robust reward schedule uses milestone-based discrete rewards for correctness and speed gains.</li>
                            <li>Anti-reward-hacking controls: protected verify/profile scripts, forbidden fallback calls, 5-input correctness checks, synchronized warm-up profiling, no web retrieval.</li>
                        </ul>
                        <p>
                            These constraints provide reliable execution-based feedback so policy learning emphasizes true
                            kernel quality rather than shortcut behaviors.
                        </p>
                        <div class="figure-wrap">
                            <img src="static/images/agent_loop.png" alt="CUDA Agent environment loop">
                        </div>
                    </div>
                    <div class="card">
                        <h3>Training Pipeline</h3>
                        <p>
                            Training is staged to stabilize long-horizon RL for CUDA coding. We first run single-turn PPO warm-up,
                            then initialize both actor and critic before full multi-turn agentic RL.
                        </p>
                        <ul class="pipeline-list">
                            <li>Single-turn warm-up improves base CUDA generation before entering interactive agent training.</li>
                            <li>Actor initialization uses Rejection Fine-Tuning (RFT) on sampled trajectories with positive outcomes.</li>
                            <li>RFT filtering removes inefficient loops and invalid tool-call patterns to reduce policy collapse risk.</li>
                            <li>Critic initialization uses value pretraining so advantage estimates are reliable from early steps.</li>
                        </ul>
                        <p>
                            With this multi-stage design, training remains stable for long-context settings (up to 128k context,
                            150 training turns, and up to 200 turns during evaluation), enabling sustained reward growth.
                        </p>
                        <div class="figure-wrap">
                            <img src="static/images/training_stages.png" alt="CUDA Agent training stages">
                        </div>
                    </div>
                </div>
            </section>

            <section id="main-results">
                <h2>Main Results</h2>
                <p>
                    We report full metrics for both Overall and Level-3 splits on KernelBench:
                    Pass Rate, Faster Rate (vs. Eager / vs. Compile), and Geomean Speed-up (vs. Eager / vs. Compile).
                </p>
                <div class="metric-columns">
                    <div class="metric-panel overall">
                        <h3>Overall</h3>
                        <div class="metric-row">
                            <span class="label">Pass Rate</span>
                            <span class="value">98.8%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Faster Rate vs. Eager</span>
                            <span class="value">98.4%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Faster Rate vs. Compile</span>
                            <span class="value">96.8%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Speed-up vs. Eager</span>
                            <span class="value">2.60x</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Speed-up vs. Compile</span>
                            <span class="value">2.11x</span>
                        </div>
                    </div>

                    <div class="metric-panel level3">
                        <h3>Level-3</h3>
                        <div class="metric-row">
                            <span class="label">Pass Rate</span>
                            <span class="value">94%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Faster Rate vs. Eager</span>
                            <span class="value">94%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Faster Rate vs. Compile</span>
                            <span class="value">90%</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Speed-up vs. Eager</span>
                            <span class="value">1.80x</span>
                        </div>
                        <div class="metric-row">
                            <span class="label">Speed-up vs. Compile</span>
                            <span class="value">1.52x</span>
                        </div>
                    </div>
                </div>
                <div class="results-note">
                    Compared with strong proprietary baselines, CUDA Agent shows a clear optimization gap in compile-relative
                    performance: on overall KernelBench, it reaches 96.8% faster rate vs. compile and 2.11x geomean speed-up,
                    while Claude Opus 4.5 and Gemini 3 Pro are around 66.4%-69.6% faster rate and 1.42x-1.46x speed-up.
                    The advantage is most pronounced on difficult settings: on Level-3, CUDA Agent achieves 90% faster rate
                    vs. compile (about +40 points over the strongest proprietary baselines), and on Level-2 operator-sequence
                    tasks it reaches 100% faster rate with 2.80x geomean speed-up vs. compile.
                </div>
                <div class="figure-wrap">
                    <img src="static/images/main_results.png" alt="Main experimental results on KernelBench">
                    <p class="figure-caption">Overall performance and speedup metrics on KernelBench.</p>
                </div>
            </section>

            <section id="citation">
                <h2>Citation</h2>
                <p>If you use CUDA Agent in your research, please cite:</p>
                <div class="citation-box">
<pre>@article{cudaagent2026,
  title   = {CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation},
  author  = {Dai, Weinan and Wu, Hanlin and Yu, Qiying and Gao, Huan-ang and Li, Jiahao and Jiang, Chengquan and Lou, Weiqiang and Song, Yufan and Yu, Hongli and Chen, Jiaze and Ma, Wei-Ying and Zhang, Ya-Qin and Liu, Jingjing and Wang, Mingxuan and Liu, Xin and Zhou, Hao},
  journal = {arXiv preprint},
  year    = {2026}
}</pre>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <p>¬© CUDA Agent Team. Page style adapted from the FLEX project template.</p>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>
